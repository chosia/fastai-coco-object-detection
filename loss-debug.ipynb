{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.core import *\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pdb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco = untar_data(URLs.COCO_SAMPLE)\n",
    "\n",
    "img_dir = coco/'train_sample'\n",
    "annotations = coco/'annotations'/'train_sample.json'\n",
    "\n",
    "#image size\n",
    "sz = 224\n",
    "\n",
    "with open(annotations) as f:\n",
    "    train_json = json.load(f)\n",
    "ncat = len(train_json['categories'])\n",
    "\n",
    "images, lbl_bbox = get_annotations(annotations)\n",
    "\n",
    "img2bbox = dict(zip(images, lbl_bbox))\n",
    "get_y_func = lambda o:img2bbox[o.name]\n",
    "\n",
    "data = (ObjectItemList.from_folder(coco)\n",
    "        #Where are the images? -> in coco and its subfolders\n",
    "        .split_by_rand_pct(valid_pct=0.1, seed=0)                          \n",
    "        #How to split in train/valid? -> randomly with the default 20% in valid\n",
    "        .label_from_func(get_y_func)\n",
    "        #How to find the labels? -> use get_y_func on the file name of the data\n",
    "        .transform(get_transforms(), size=sz, tfm_y=True)\n",
    "        #Data augmentation? -> Standard transforms; also transform the label images\n",
    "        .databunch(bs=8, collate_fn=bb_pad_collate))   \n",
    "        #Finally we convert to a DataBunch, use a batch size of 16,\n",
    "        # and we use bb_pad_collate to collate the data into a mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StdConv(nn.Module):\n",
    "    def __init__(self, nin, nout, stride=2, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(nin, nout, 3, stride=stride, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(nout)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, x): return self.drop(self.bn(F.relu(self.conv(x))))\n",
    "\n",
    "def flatten_conv(x,k):\n",
    "    bs,nf,gx,gy = x.size()\n",
    "    x = x.permute(0,2,3,1).contiguous()\n",
    "    return x.view(bs,-1,nf//k)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, k, nin, bias):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.oconv1 = nn.Conv2d(nin, 4*k, 3, padding=1) # first bboxes\n",
    "        self.oconv2 = nn.Conv2d(nin, (ncat+1)*k, 3, padding=1) # than class labels\n",
    "        self.oconv2.bias.data.zero_().add_(bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return [flatten_conv(self.oconv1(x), self.k),\n",
    "                flatten_conv(self.oconv2(x), self.k)]\n",
    "\n",
    "class SSD_Head(nn.Module):\n",
    "    def __init__(self, k, bias):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(0.25)\n",
    "        self.sconv0 = StdConv(512,256, stride=1)\n",
    "#         self.sconv1 = StdConv(256,256)\n",
    "        self.sconv2 = StdConv(256,256)\n",
    "        self.out = OutConv(k, 256, bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.drop(F.relu(x))\n",
    "        x = self.sconv0(x)\n",
    "#         x = self.sconv1(x)\n",
    "        x = self.sconv2(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centre+height/width -> corners\n",
    "def hw2corners(ctr, hw): return torch.cat([ctr-hw/2, ctr+hw/2], dim=1)\n",
    "\n",
    "def create_anchors(ncells=4, k=1):\n",
    "    # create a tensor with anchor boxes - middles + sizes\n",
    "    # coordinates of bboxes are scaled to -1,1, so anchor boxes must be too\n",
    "    # ncells - ncells in a grid dimension\n",
    "    # k - num boxes per cell\n",
    "    first_ctr = -1 + 2/(2*ncells)\n",
    "    last_ctr  =  1 - 2/(2*ncells)\n",
    "    a_x = np.repeat(np.linspace(first_ctr, last_ctr, ncells), ncells)\n",
    "    a_y = np.tile(np.linspace(first_ctr, last_ctr, ncells), ncells)\n",
    "    a_sz = np.array([2/ncells for _ in a_x])\n",
    "    anchors = torch.tensor(np.stack([a_x, a_y, a_sz, a_sz], axis=1)).type(torch.FloatTensor).cuda()\n",
    "    anchor_cnr = hw2corners(anchors[:,:2], anchors[:,2:]) # anchor boxes corners\n",
    "    grid_sizes = torch.tensor(np.array([2/ncells]), requires_grad=False).type(torch.FloatTensor).unsqueeze(1).cuda()\n",
    "    return anchors, anchor_cnr, grid_sizes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors, anchor_cnr, grid_sizes = create_anchors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(box_a, box_b):\n",
    "    max_xy = torch.min(box_a[:, None, 2:], box_b[None, :, 2:])\n",
    "    min_xy = torch.max(box_a[:, None, :2], box_b[None, :, :2])\n",
    "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]\n",
    "\n",
    "def box_sz(b): return ((b[:, 2]-b[:, 0]) * (b[:, 3]-b[:, 1]))\n",
    "\n",
    "def jaccard(box_a, box_b):\n",
    "    inter = intersect(box_a, box_b)\n",
    "    union = box_sz(box_a).unsqueeze(1) + box_sz(box_b).unsqueeze(0) - inter\n",
    "    return inter / union\n",
    "\n",
    "def map_to_ground_truth(overlaps, print_it=False):\n",
    "    prior_overlap, prior_idx = overlaps.max(1)\n",
    "    if print_it: print(prior_overlap)\n",
    "#     pdb.set_trace()\n",
    "    gt_overlap, gt_idx = overlaps.max(0)\n",
    "    gt_overlap[prior_idx] = 1.99\n",
    "    for i,o in enumerate(prior_idx): gt_idx[o] = i\n",
    "    return gt_overlap,gt_idx\n",
    "\n",
    "def actn_to_bb(actn, anchors):\n",
    "    actn_bbs = torch.tanh(actn)\n",
    "    actn_centers = (actn_bbs[:,:2]/2 * grid_sizes) + anchors[:,:2]\n",
    "    actn_hw = (actn_bbs[:,2:]/2+1) * anchors[:,2:]\n",
    "    return hw2corners(actn_centers, actn_hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_embedding(labels, num_classes):\n",
    "    return torch.eye(num_classes)[labels.data.cpu()]\n",
    "\n",
    "class BCE_Loss(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, pred, targ):\n",
    "        t = one_hot_embedding(targ, self.num_classes+1)\n",
    "#         t = t[:,:-1].contiguous().cuda()\n",
    "#         x = pred[:,:-1]\n",
    "        # in fastai v1, the background is encoded as the first class, rather than the last one\n",
    "        t = t[:,1:].contiguous().cuda()\n",
    "        x = pred[:,1:]\n",
    "        w = self.get_weight(x,t)\n",
    "        return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)/self.num_classes\n",
    "    \n",
    "    def get_weight(self,x,t): return None\n",
    "\n",
    "loss_f = BCE_Loss(ncat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove padding - images in a minibatch will have a different number of objects\n",
    "# those with fewer objects will have their bbox groundtruth tensor padded with 0s\n",
    "def get_y(bbox,clas):\n",
    "    bbox = bbox.view(-1,4)\n",
    "    bb_keep = ((bbox[:,2]-bbox[:,0])>0).nonzero()[:,0]\n",
    "    return bbox[bb_keep],clas[bb_keep]\n",
    "\n",
    "def ssd_1_loss(pred_bbox,pred_clas,bbox,clas):\n",
    "    bbox,clas = get_y(bbox,clas)\n",
    "    if len(bbox) == 0 and len(clas) == 0:\n",
    "        return 0.0, 0.0\n",
    "    a_ic = actn_to_bb(pred_bbox, anchors)\n",
    "    overlaps = jaccard(bbox.data, anchor_cnr.data)\n",
    "    gt_overlap,gt_idx = map_to_ground_truth(overlaps,False)\n",
    "    gt_clas = clas[gt_idx]\n",
    "    pos = gt_overlap > 0.4\n",
    "    pos_idx = torch.nonzero(pos)[:,0]\n",
    "    gt_clas[~pos] = 0 # background coded as 0\n",
    "    gt_bbox = bbox[gt_idx]\n",
    "    print(a_ic[pos_idx] - gt_bbox[pos_idx])\n",
    "    loc_loss = ((a_ic[pos_idx] - gt_bbox[pos_idx]).abs()).mean()\n",
    "    clas_loss  = loss_f(pred_clas, gt_clas)\n",
    "    print(f\"loc_loss: {loc_loss}, clas_loss: {clas_loss}\")\n",
    "    return loc_loss, clas_loss\n",
    "\n",
    "def ssd_loss(pred,targ_bb,targ_c):\n",
    "    lcs,lls = 0.,0.\n",
    "#    i = 1\n",
    "#    for b_c,b_bb,bbox,clas in zip(*pred,targ_bb,targ_c):\n",
    "    for pr_bb, pr_c, ta_bb, ta_c in zip(*pred, targ_bb, targ_c):\n",
    "#         print(f\"item {i}\")\n",
    "#         if i == 5:\n",
    "#             pdb.set_trace()\n",
    "#         i = i+1\n",
    "#        loc_loss,clas_loss = ssd_1_loss(b_c,b_bb,bbox,clas)\n",
    "        loc_loss, clas_loss = ssd_1_loss(pr_bb, pr_c, ta_bb, ta_c)\n",
    "        lls += loc_loss\n",
    "        lcs += clas_loss\n",
    "#    if print_it: print(f'loc: {lls.data[0]}, clas: {lcs.data[0]}')\n",
    "    return 30*lls+lcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_reg4 = SSD_Head(k=1, bias=-3.)\n",
    "learn = cnn_learner(data, models.resnet34, loss_func=ssd_loss, custom_head=head_reg4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('better-model-loc-lossx30');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (ObjectItemList.from_folder(coco)\n",
    "        .split_by_rand_pct(valid_pct=0.1, seed=0)                          \n",
    "        .label_from_func(get_y_func)\n",
    "        .transform(get_transforms(), size=sz, tfm_y=True)\n",
    "        .databunch(bs=8, collate_fn=bb_pad_collate))   \n",
    "x,y = data.one_batch(ds_type=DatasetType.Valid)\n",
    "x = x.cuda()\n",
    "y_bb, y_c = y\n",
    "y_bb = y_bb.cuda()\n",
    "y_c = y_c.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = learn.model(x)\n",
    "y_pred_bb, y_pred_c = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "y_bb_i = y_bb[i].unsqueeze(0)\n",
    "y_c_i  = y_c[i].unsqueeze(0)\n",
    "y_pred_i = (y_pred_bb[i].unsqueeze(0), y_pred_c[i].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1034, -0.0976,  0.5972,  0.4994],\n",
      "        [ 0.2693,  0.3305,  0.5311,  0.4556],\n",
      "        [ 0.3235, -0.4616,  0.4421, -0.1691]], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n",
      "loc_loss: 0.3567039370536804, clas_loss: 4.574733257293701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gosia/fastai/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(15.2759, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%debug\n",
    "ssd_loss(y_pred_i, y_bb_i, y_c_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5708, -0.2397,  0.4113, -0.0435],\n",
      "        [ 0.2182,  0.5587, -0.0846,  0.4222],\n",
      "        [ 0.0717,  0.5275, -0.1957,  0.3994]], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n",
      "loc_loss: 0.3119214177131653, clas_loss: 2.773035764694214\n",
      "tensor([[-0.1034, -0.0976,  0.5972,  0.4994],\n",
      "        [ 0.2693,  0.3305,  0.5311,  0.4556],\n",
      "        [ 0.3235, -0.4616,  0.4421, -0.1691]], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n",
      "loc_loss: 0.3567039370536804, clas_loss: 4.574733257293701\n",
      "tensor([[ 0.7013,  0.1199, -0.6790, -1.6278]], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n",
      "loc_loss: 0.7819737792015076, clas_loss: 2.022991180419922\n",
      "tensor([[-0.3065, -0.5251, -0.1677, -0.3485],\n",
      "        [-0.0179, -0.7334,  0.1514, -0.0897]], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n",
      "loc_loss: 0.29253679513931274, clas_loss: 3.372897148132324\n",
      "tensor([[-0.0219,  0.3872,  0.1059,  0.5014],\n",
      "        [-0.0199,  0.1155, -0.0303,  0.2087]], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n",
      "loc_loss: 0.17383047938346863, clas_loss: 0.8445405960083008\n",
      "tensor([[-0.4368,  0.1506, -0.0930,  0.1384]], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n",
      "loc_loss: 0.20470330119132996, clas_loss: 1.6043951511383057\n",
      "tensor([[ 0.0995, -0.4975,  0.4325,  0.0006],\n",
      "        [ 0.3261, -0.3230,  0.4538, -0.2015],\n",
      "        [ 0.3386, -0.1403,  0.5146,  0.1269],\n",
      "        [-0.3088, -0.0414,  0.2331,  0.2910]], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n",
      "loc_loss: 0.27058708667755127, clas_loss: 5.422337532043457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(92.3826, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssd_loss(y_pred, y_bb, y_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
